# @configure_input@                                     -*- shell-script -*-
# atlocal.in for UniversalCodeGrep's ./tests directory, used to collect configure-time
# information about the build system.
#
# Copyright 2015-2016 Gary R. Van Sickle (grvs@users.sourceforge.net).
#
# This file is part of UniversalCodeGrep.
#
# UniversalCodeGrep is free software: you can redistribute it and/or modify it under the
# terms of version 3 of the GNU General Public License as published by the Free
# Software Foundation.
#
# UniversalCodeGrep is distributed in the hope that it will be useful, but WITHOUT ANY
# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
# PARTICULAR PURPOSE.  See the GNU General Public License for more details.
# 
# You should have received a copy of the GNU General Public License along with
# UniversalCodeGrep.  If not, see <http://www.gnu.org/licenses/>.

## Process this file with autoconf to produce atlocal.

# We need this so we're able to find the binary we built.
PATH=@abs_builddir@:@abs_top_builddir@/src:@abs_top_srcdir@/build-aux:$top_srcdir:$srcdir:$PATH

XFAILFILE=$abs_builddir/.badversion

trap "test -r $XFAILFILE && cat $XFAILFILE; exit $?" 1 2 13 15

# At testsuite-time, find the programs we wish to compare performance with.
PERF_PROGRAMS_TO_COMPARE=ucg
if PROG_AG="$(which ag)";
then
	PERF_PROGRAMS_TO_COMPARE=$PERF_PROGRAMS_TO_COMPARE" "$PROG_AG;
fi
#if PROG_ACK="$(which ack)";
#then
#	PERF_PROGRAMS_TO_COMPARE=$PERF_PROGRAMS_TO_COMPARE" "$PROG_ACK;
#fi

# Paths to source that we'll test against.
# @todo Maybe take this in as a parameter?  The only issue is that this doesn't work during a "make distcheck",
# which arguably is not a big deal.
BOOST_PATH=${abs_top_srcdir}/../boost_1_58_0

# The file where we'll put the results of the performance tests.
# This file will be created by testsuite.at.
PERF_RESULTS_FILE=${abs_top_builddir}/tests/perf_test_results.txt

PROG_SCRIPT="@PROG_SCRIPT@"
PROG_SCRIPT_TYPE="@PROG_SCRIPT_TYPE@"
PROG_SCRIPT_PRE_TEXT="@PROG_SCRIPT_PRE_TEXT@"
PROG_SCRIPT_POST_TEXT="@PROG_SCRIPT_POST_TEXT@"
ESED="@ESED@"
MKDIR_P="@MKDIR_P@"
TEST_LN_S="@TEST_LN_S@"

### Functions for use in the testsuite.

get_dev_and_fs_type()
{
	# Get, in as portable a way as possible, the device and filesystem type on which the
	# file passed in $1 resides.
	mount | fgrep -w "`df \"$1\" | grep '%' | sed -e 's/.*% *//'`"
}

ASX_SCRIPT ()
{
	# Note: '\r' removal in here because script outputs \r\n's, even on Linuxes.
	case $PROG_SCRIPT_TYPE in
		linux) $PROG_SCRIPT $PROG_SCRIPT_PRE_TEXT "$*" $PROG_SCRIPT_POST_TEXT | tr -d '\r';;
		bsd) $PROG_SCRIPT $PROG_SCRIPT_PRE_TEXT "${@}" $PROG_SCRIPT_POST_TEXT | tr -d '\r';;
		*) exit 1;;
	esac
}

# Make time only output elapsed time in seconds.
TIMEFORMAT=%R

NUM_ITERATIONS=10

declare -a AVG_TIME
declare -a PREP_RUN_FILES
RESULTS_FILE=$PERF_RESULTS_FILE

run_test_cmdlines()
{
	declare -a CMD_LINE_ARRAY;

	# Load the command lines from the filename passed as arg 1.	
	IFS=$'\n' read -d '' -r -a CMD_LINE_ARRAY < "${1}"
	
	i=0;
	
	echo "Start" >> "${RESULTS_FILE}"
	
	for COMMAND_LINE in "${CMD_LINE_ARRAY[@]}";
	do
		echo "Trying: $COMMAND_LINE" >> "${RESULTS_FILE}";
		# "Prep" run, to eliminate disk cache variability and capture the matches.
		# We pipe the results through sort so we can diff these later. 
		PREP_RUN_FILES[$i]="SearchResults_${i}.txt"
		echo "Prep run for command line: '${COMMAND_LINE}'" > SearchResults_${i}.txt; 
		{ ( eval "${COMMAND_LINE[@]}" 2>&1 ); } 3>&1 4>&2 | sort >> SearchResults_${i}.txt;
	
		# Timing runs.
		for ITER in `seq 0 $(($NUM_ITERATIONS - 1))`; do
			#{ REAL_TIME[$ITER]=$( { time ${PROG} ${PARAM_LIST[@]} ${REGEX} ${TEST_DATA_DIR} 1>&3- 2>&4-; } 2>&1 ); } 3>&1 4>&2;
			{ REAL_TIME[$ITER]=$( eval ${COMMAND_LINE} 2>&1 ); } 3>&1 4>&2;
			echo "${REAL_TIME[$ITER]}" >> "${RESULTS_FILE}";
		done
	
		# Determine the average.
		AVG_TIME[i]=0;
		for ELAPSED in ${REAL_TIME[@]}; do
			AVG_TIME[i]=$(echo "${AVG_TIME[i]} + $ELAPSED" | bc -ls);
		done
		AVG_TIME[i]=$(echo "${AVG_TIME[i]} / $NUM_ITERATIONS" | bc -ls);
		echo "Average elapsed time: ${AVG_TIME[i]}" >> "${RESULTS_FILE}";
		((++i));
	done
	
	# Output the results.
	echo "| Program | Avg of ${NUM_ITERATIONS} runs |" >> "${RESULTS_FILE}";
	echo "|---------|---------------|" >> "${RESULTS_FILE}";
	for i in `seq 0 $((${#CMD_LINE_ARRAY[@]} - 1))`; ### ((i=0;i<${#CMD_LINE_ARRAY[@]}; ++i));
	do
		echo "| ${CMD_LINE_ARRAY[i]} | ${AVG_TIME[i]} |" >> "${RESULTS_FILE}";
	done
}

perf_test_1()
{

PROG_WDIFF=wdiff

RESULTS_FILE=$PERF_RESULTS_FILE

# Make time only output elapsed time in seconds.
TIMEFORMAT=%R

NUM_ITERATIONS=10

declare -a AVG_TIME
declare -a PREP_RUN_FILES



diff_prep_run_output()
{
	declare -a PREP_RUN_OUTPUT_FILENAMES;
	PREP_RUN_OUTPUT_FILENAMES=("${!1}")
	declare -a CMD_LINE_ARRAY;
	CMD_LINE_ARRAY=("${!2}")
	i=0;
	
	# Use the grep results as the standard.
	GOLD_STD_FILE="${PREP_RUN_OUTPUT_FILENAMES[-1]}"
	
	for NAME in "${PREP_RUN_OUTPUT_FILENAMES[@]}";
	do
		#echo $NAME;
		NUM_MATCHED_LINES[i]=$(cat "${NAME}" | tail -n +3 | wc -l );
		WDIFF_OUT="$($PROG_WDIFF -123 -s  <(tail -n +3 "${GOLD_STD_FILE}") <(tail -n +3 "${NAME}") | tr -s ' ')";
		# filename of second file now is field 13.
		WORDS_COMMON=$(echo $WDIFF_OUT | cut -d' ' -f16);
		WORDS_DELETED=$(echo $WDIFF_OUT | cut -d' ' -f19);
		WORDS_COMMON_PCT[i]=$(echo $WDIFF_OUT | cut -d' ' -f17)
		#echo "WDIFF_OUT = '${WDIFF_OUT}'";
		((i++))
	done;
	
	# diff -u0 <(tail -n +3 SearchResults_1.txt) <(tail -n +3 SearchResults_10.txt) | diffstat -Es
	# Output will be:
	# " 0 files changed"
	# " 1 file changed, 19473 deletions(-)"
	# wdiff -123 -s  <(tail -n +3 SearchResults_1.txt) <(tail -n +3 SearchResults_10.txt)
	# Output will be:
	# /dev/fd/63: 44357 words  44357 100% common  0 0% deleted  0 0% changed
	# /dev/fd/62: 44357 words  44357 100% common  0 0% inserted  0 0% changed
	# Or with empty file:
	# /dev/fd/63: 44357 words  0 0% common  44357 100% deleted  0 0% changed
	# /dev/fd/62: 0 words

	echo "| Command line | Total matched lines count | Percent of output words common |" >> "${RESULTS_FILE}";
	echo "|--------------|---------------------------|--------------------------------|" >> "${RESULTS_FILE}";
	for i in `seq 0 $((${#CMD_LINE_ARRAY[@]} - 1))`;
	do
		echo "| ${CMD_LINE_ARRAY[i]} | ${NUM_MATCHED_LINES[i]} | ${WORDS_COMMON_PCT[i]} |" >> "${RESULTS_FILE}";
	done

}


declare -a VAR

#mkfifo cmd_line_pipe
#generate_cmdlines_ucgjobs_compare > cmd_line_pipe &
#awk -f $srcdir/performance_test_gen.awk > $abs_builddir/cmdlines.txt
#awk -f $srcdir/performance_test_gen.awk > cmd_line_pipe &

echo here6 >> $PERF_RESULTS_FILE

#mapfile -t VAR < cmd_line_pipe

echo here7 >> $PERF_RESULTS_FILE

#for V in "${VAR[@]}"; do
#	echo ">> $V" >> "${RESULTS_FILE}";
#done

#run_test_cmdlines cmd_line_pipe ###VAR[@]

#diff_prep_run_output PREP_RUN_FILES[@] VAR[@]
}

