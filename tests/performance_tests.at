# performance_tests.at for UniversalCodeGrep
#
# Copyright 2015-2016 Gary R. Van Sickle (grvs@users.sourceforge.net).
#
# This file is part of UniversalCodeGrep.
#
# UniversalCodeGrep is free software: you can redistribute it and/or modify it under the
# terms of version 3 of the GNU General Public License as published by the Free
# Software Foundation.
#
# UniversalCodeGrep is distributed in the hope that it will be useful, but WITHOUT ANY
# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A
# PARTICULAR PURPOSE.  See the GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License along with
# UniversalCodeGrep.  If not, see <http://www.gnu.org/licenses/>.


### Filenames.  We do this with m4_defines so that ${builddir} etc. don't get expanded
### outside of the testcase, which would result in the path being incorrect.  Remember that the
### testcases are run in ./tests/testsuite.dir/NN/, but outside a test case, e.g. ${builddir} is ".", not
### the "../.." that we really want.
###
### Directories during test time:
###
### AS_ECHO([Dirs: pwd=$(pwd) , srcdir=$srcdir , at_top_srcdir=$at_top_srcdir , top_srcdir=$top_srcdir , abs_builddir=$abs_builddir builddir=$builddir,$(realpath $builddir)]) >> UCG_PERF_RESULTS_FILE
### AT_CHECK([AS_ECHO "srcdir: ${srcdir} == $(realpath ${srcdir})" > diags.txt], [0])
### AT_CHECK([AS_ECHO "at_srcdir: ${at_srcdir} == $(realpath ${at_srcdir})" >> diags.txt], [0])
### AT_CHECK([AS_ECHO "builddir: ${builddir} == $(realpath ${builddir})" >> diags.txt], [0])
### AT_CAPTURE_FILE([diags.txt])
###
### pwd=/<...>/UCGTopSrcDir/build/tests/testsuite.dir/39
### srcdir=../../../../tests <== This will get you to the srcdir of the test's *.at file at test run time.
### top_srcdir=../../../.. <== This will get you to the real top_srcdir relative to the test's CWD at test runtime.
### builddir=../.. <== This will get you to /<..>/UCGTopBuildDir/tests from the CWD at test runtime.
### abs_builddir=/<...>/UCGTopSrcDir/build/tests
### at_top_srcdir=../..
###

# Paths to source that we'll test against.
## We generate this file.
m4_define([UCG_TEST_FILE_NAME_LARGE_FILE_1], ["${builddir}/500MBLoremIpsum.cpp"])
## The Boost library, version 1.58.0, as unpacked from the tarball.
m4_define([UCG_CORPUS_BOOST_PATH], ["${top_srcdir}/../boost_1_58_0"])
## A built linux source tree.
m4_define([UCG_CORPUS_LINUX_PATH], ["${top_srcdir}/../TestCorpus/linux"])

# The file where we'll put the results of the benchmark tests.
# This file will be created in performance_tests.at.
m4_define([UCG_PERF_RESULTS_FILE], [${builddir}/perf_test_results.txt])

# Preference list for choosing a prep run output file to use as the matched-lines standard.
# Find the one with the most lines, preferring GNU grep -P, then GNU grep -E, then the system grep.
m4_define([UCG_PREF_LIST], [inst_system_grep
inst_gnu_grep_e
inst_gnu_grep_p])

# "source" is neither POSIX nor portable.  Macro so we can find uses of the portable "." if we need to.
m4_define([AXUCG_SOURCE], [.])

###
### Start of the benchmark tests.
### @todo Probably should change terminology to "benchmarks" throughout.
###
AT_BANNER([UniversalCodeGrep Benchmarks])

#
# Create the logfile.
#
AT_SETUP([Create report log file])
AT_KEYWORDS([benchmark])
AS_ECHO(["ucg Benchmark Test Results"]) > UCG_PERF_RESULTS_FILE
AS_ECHO(["Test run started at: `date '+%Y-%m-%d %T%z' | sed 's/\(..\)\(..\)$/\1:\2/'`"]) >> UCG_PERF_RESULTS_FILE
AT_CLEANUP

#
# Record some system info.
#
AT_SETUP([Recording system info])
AT_KEYWORDS([benchmark])
AS_ECHO(["START SYSTEM INFO"]) >> UCG_PERF_RESULTS_FILE
AT_CHECK([get_system_info >> UCG_PERF_RESULTS_FILE], [0], [stdout], [stderr])
AS_ECHO(["END SYSTEM INFO"]) >> UCG_PERF_RESULTS_FILE
AT_CLEANUP

#
# Get the versions of the programs we're comparing to.
#
AT_SETUP([Getting program versions])
AT_KEYWORDS([benchmark])

AS_ECHO(["START PROGVER INFO"]) >> UCG_PERF_RESULTS_FILE
# Loop through the programs we're comparing performance with.
PROGLIST_TUPLES=$(AS_ECHO(["$PROGLIST"]) | $SED 's/,/ /g')
for TUPLE in $PROGLIST_TUPLES; do
	TEST_PROG_ID=$(AS_ECHO(["$TUPLE"]) | $ESED -n 's/(.*):.*$/\1/p')
	TEST_PROG_PATH=$(AS_ECHO(["$TUPLE"]) | $ESED -n 's/.*:(.*)$/\1/p')
	AS_ECHO([PROGVER_START]) >> UCG_PERF_RESULTS_FILE
	AS_ECHO(["TEST_PROG_ID: $TEST_PROG_ID"]) >> UCG_PERF_RESULTS_FILE
	AS_ECHO(["TEST_PROG_PATH: $TEST_PROG_PATH"]) >> UCG_PERF_RESULTS_FILE
	AS_ECHO(["$TEST_PROG_PATH --version:"]) >> UCG_PERF_RESULTS_FILE
	AS_ECHO(["$($TEST_PROG_PATH --version)"]) >> UCG_PERF_RESULTS_FILE
	AS_ECHO([PROGVER_END]) >> UCG_PERF_RESULTS_FILE
done;
AS_ECHO(["END PROGVER INFO"]) >> UCG_PERF_RESULTS_FILE

AT_CLEANUP


###
### UCG_SUMMARIZE_PERFTEST
### M4 macro which generates the shell script code to parse and summarize the results of a single benchmark run.
###
m4_define([UCG_SUMMARIZE_PERFTEST],
[
# Choose a prep run output file to use as the standard.
# Find the one with the most lines, preferring GNU grep then the system grep.
PREF_LIST="UCG_PREF_LIST";

pref_index()
{
	retval=$(AS_ECHO "${PREF_LIST}" | $ESED -n "/$[1]/=");
	if test "x$retval" = "x"; then retval=0; fi;
	AS_ECHO $retval;
}

MAX_LINE_FN="";
MAX_LINECT=0;
MAX_PROG_ID="";
MAX_PREF_VAL=0;
for fn in $(ls SearchResults_*.txt); do
	# Get the '*' we matched on.
	WILDCARD=$(AS_ECHO $fn | $ESED -n 's/SearchResults_(.*)\.txt/\1/p');

	# Create a temp match file with the header and time triplet cut off.
	cat $fn | $ESED -n '1,/END OF HEADER/! p' | tail -n +4 > temp_$fn;

	# Count the number of lines matched.
	LINECT=$(cat temp_$fn | LCT)

	# Tack the line count onto the associated timing results list.
	AS_ECHO "NUM_MATCHED_LINES: ${LINECT}" >> time_results_${WILDCARD}.txt;

	# Find the prog_id which generated these results.
	THIS_PROG_ID=$($AWK '$[1] ~ /^TEST_PROG_ID:/ { print $[2]; }' $fn);
	THIS_PREF_VAL=$(pref_index $THIS_PROG_ID);
	AS_ECHO "PREF_INDEX for $THIS_PROG_ID: $(pref_index $THIS_PROG_ID)" >> UCG_PERF_RESULTS_FILE
	AS_IF([test "$THIS_PREF_VAL" -gt 0 && test "$THIS_PREF_VAL" -ge "$MAX_PREF_VAL"],
		[
		AS_IF([test "$LINECT" -ge "$MAX_LINECT"],
			[
				MAX_LINECT=$LINECT;
				MAX_LINE_FN=$fn;
				MAX_PROG_ID=$THIS_PROG_ID;
				MAX_PREF_VAL=$THIS_PREF_VAL;
			])
		])
done;
AS_ECHO "Note: Max lines in file \"$MAX_LINE_FN\" from $MAX_PROG_ID: $MAX_LINECT" >> UCG_PERF_RESULTS_FILE;

# Now, diff the results files.
for fn in $(ls temp_SearchResults_*.txt); do
	# Get the '*' we matched on.
	WILDCARD=$(AS_ECHO $fn | $ESED -n 's/temp_SearchResults_(.*)\.txt/\1/p');

	# Diff against the file selected above.
	NUM_CHARS_DIFF=$(git diff --no-index --word-diff=porcelain $fn $MAX_LINE_FN | $EGREP '^(\\+[[^+]])|^(-[[^-]])' | wc -m);

	# Tack the line count onto the associated timing results list.
	AS_ECHO "NUM_CHARS_DIFF: ${NUM_CHARS_DIFF}" >> time_results_${WILDCARD}.txt;
done;

# Output the results.
AS_ECHO "| Program | Avg of @todo runs | Sample Stddev | SEM | Num Matched Lines | Num Diff Chars |" >> UCG_PERF_RESULTS_FILE;
AS_ECHO "|---------|----------------|---------------|-----|-------------------|---|" >> UCG_PERF_RESULTS_FILE;
for fn in $(ls -1 time_results_*.txt); do
	AT_CHECK([${srcdir}/stats.awk $fn >> UCG_PERF_RESULTS_FILE], [0], [stdout], [stderr])
done;
])


###
### ATUCG_INSTANTIATE_ONE_BENCHMARK
### M4 macro which expands to a single AutoTest AT_SETUP/AT_CHECK/AT_CLEANUP benchmark test case.
###
# $1 = Test case identifier (e.g. "TC1").
# $2 = Test case title.
# $3 = Directory which must exist or skip test.
m4_define([ATUCG_INSTANTIATE_ONE_BENCHMARK],
[
AT_SETUP(m4_esyscmd_s(echo $2))dnl The echo here is to "shell-unescape" the shell-escaped title.
AT_KEYWORDS([benchmark])

# Skip this test if the test script reports it's not ready.  The most likely cause is that we can't find the necessary test corpus.
AT_SKIP_IF([ ! ${srcdir}/$1.sh -r])

AS_ECHO(["START PERFTEST"]) >> UCG_PERF_RESULTS_FILE

# Run the test script.
AT_CHECK([AXUCG_SOURCE ${srcdir}/$1.sh], [0], [ignore-nolog], [stderr])
# Summarize the test results.
UCG_SUMMARIZE_PERFTEST

AS_ECHO(["END PERFTEST"]) >> UCG_PERF_RESULTS_FILE

AT_CLEANUP
])


###
### Generate test files for large file tests.
###
AT_SETUP([Generating files for large file tests])
AT_KEYWORDS([benchmark])
# Generate the test file UCG_TEST_FILE_NAME_LARGE_FILE_1, which is Lorum ipsum text with one distinct test string at the end.
AT_CHECK([${builddir}/dummy-file-gen -b 500000000 | fold -s > UCG_TEST_FILE_NAME_LARGE_FILE_1], [0], [stdout], [stderr])
AT_CHECK([cat stderr | $EGREP 'Number of bytes written:'], [0], [ignore], [ignore])
AT_CHECK([echo "iudicemaequumputo" >> UCG_TEST_FILE_NAME_LARGE_FILE_1], [0], [stdout], [stderr])
AT_CLEANUP

##
## m4 experimental area.
##
#m4_errprintn(m4_esyscmd([echo "at_srcdir = ${at_srcdir}"])at_srcdir)
#m4_errprintn(m4_esyscmd([echo "srcdir = ${srcdir}"])srcdir)
#m4_errprintn(m4_esyscmd([echo "top_srcdir = ${top_srcdir}"])top_srcdir)
#m4_errprintn(m4_esyscmd([echo "abs_top_srcdir = ${abs_top_srcdir}"])abs_top_srcdir)
#m4_errprintn(m4_esyscmd([echo "pwd = $(pwd)"]))

# Extract the list of tests we need to generate from the test_cases.csv file.
# @todo pwd is build/tests here.  None of the *srcdirs seem to be defined at m4 time.
m4_define([test_case_table],
	m4_flatten(m4_esyscmd_s([cat $(pwd)/../../tests/test_cases.csv | awk 'BEGIN { FS = ",[ ]+" } /TC.*/ { printf "[[%s],[%s],]", $1, $3; }'])dnl
	)
)
#m4_define([test_case_table], m4_dquote([abc],[def gh],[ijk],[lmn opg]))
#m4_errprintn([m4_dumpdef:])
m4_dumpdef([test_case_table])

#m4_errprintn([Exp1])
#m4_define([test_case_table_1],
# m4_dquote(m4_apply([m4_echo], [test_case_table])))
#m4_dumpdef([test_case_table_1])

#m4_errprintn([Exp2])                 dnl m4_dquote(test_case_table_1)
#m4_define([mapping], m4_foreach([var], [[1],[2],[3],[4]], [echo var]))
#m4_define([mapping], m4_map_args_pair([echo], [], [abc],[def ghi],[jkl],[mno pqr stu]))
#m4_define([mapping], m4_map_args_pair([echo], [m4_ignore], m4_dquote_elt(test_case_table)))
#m4_dumpdef([mapping])
#m4_errprintn(m4_echo(mapping))
#m4_fatal([debug stop])

###
### Create the benchmark test instantiations via m4.
###
m4_map_args_pair([ATUCG_INSTANTIATE_ONE_BENCHMARK], [m4_ignore], m4_dquote_elt(test_case_table))


###
### Delete test files for the large file tests.
###
AT_SETUP([Deleting files for large file tests])
AT_KEYWORDS([benchmark])
# Delete the test file UCG_TEST_FILE_NAME_LARGE_FILE_1, which is Lorum ipsum text with one distinct test string at the end.
AT_CHECK([test -f "UCG_TEST_FILE_NAME_LARGE_FILE_1" && rm "UCG_TEST_FILE_NAME_LARGE_FILE_1"], [0], [stdout], [stderr])
AT_CLEANUP


